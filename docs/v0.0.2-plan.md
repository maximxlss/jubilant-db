# v0.0.2 ExecPlan: barebones networked transactions

This ExecPlan is a living document and must be maintained in accordance with `PLANS.md` at `/workspace/jubilant-db/PLANS.md`.

## Purpose / Big Picture

v0.0.2 demonstrates that Jubilant DB’s architecture can support remote transactions rather than only local CLI calls. A minimal server will accept simple transaction requests over the network, execute them through the existing worker and BTree plumbing, and return structured responses. A lightweight client (Python if feasible) will issue those requests, and `jubectl` will interoperate with the same flow to keep operational tooling aligned. The goal is to prove the end-to-end shape without overdesigning the protocol.

## Progress

- [x] (2025-12-17 11:59Z) Drafted the v0.0.2 ExecPlan capturing scope and acceptance.
- [x] (2025-12-17 12:35Z) Defined and documented the minimal transaction wire format and compatibility constraints in `docs/txn-wire-v0.0.2.md` (schemas, framing, and worked examples).
- [x] (2025-12-17 13:30Z) Built a Python length-prefixed echo stub plus framing probe to validate the v0.0.2 JSON envelope before wiring the C++ adapter.
- [x] (2025-12-17 12:46Z) Implemented the Python client module and CLI that speak the documented envelope for `set/get/del` (step 2).
- [x] (2025-12-17 13:30Z) Prototyped the Python client against the loopback stub server to validate framing and payloads.
- [x] (2025-12-17 12:52Z) Implemented the C++ server network adapter that decodes TCP JSON frames into `TransactionRequest` objects, dispatches them through `Server::SubmitTransaction`, and writes back `TransactionResult` frames.
- [x] (2025-12-17 16:29Z) Added `jubectl --remote`/`txn` paths that speak the JSON envelope for parity checks and reuse a shared framing helper.
- [x] (2025-12-17 14:20Z) Added the `jubildb_server` bootstrap that loads TOML config, initializes storage metadata, starts workers, and wires sockets into the network adapter (step 5).
- [x] (2025-12-17 18:18Z) Added CMake targets and presets that build the network server and stage Python client scripts into the build tree (step 7).
- [ ] Write integration tests demonstrating set/get/delete round-trips through the network stack.

## Surprises & Discoveries

- Observation: The server needed a condition variable to avoid busy-polling for completed transactions when streaming responses to clients.
  Evidence: Added `Server::WaitForResults` and used it from the network dispatch loop to sleep until results arrive.
- Observation: A frame size cap was necessary to keep malformed prefixes from allocating excessive buffers.
  Evidence: Network adapter enforces a 1 MiB maximum and closes connections that exceed it, with the limit documented in `docs/txn-wire-v0.0.2.md`.
- Observation: Standard-library sockets and `struct` keep the length-prefixed framing simple; an 8 MiB safety cap protects the stub from malformed length prefixes while still covering the documented payload sizes.
  Evidence: `tools/clients/python/echo_stub.py` enforces `MAX_FRAME_SIZE_BYTES` and rejects oversized frames early.

## Decision Log

- Decision: Start with a length-prefixed JSON payload over TCP for transaction requests/responses to minimize dependencies while keeping room to swap to FlatBuffers later.
  Rationale: JSON keeps the prototype simple and debuggable, allows Python and C++ implementations without extra generators, and mirrors the in-memory `TransactionRequest` fields closely enough for this milestone.
  Date/Author: 2025-12-17 / GPT-5.1-Codex-Max.
- Decision: Use `nlohmann::json` for request/response translation and cap frames at 1 MiB to bound resource use while honoring the documented schema.
  Rationale: The header-only JSON dependency keeps parsing/serialization straightforward for both server and tests, and the cap guards against malicious prefixes during the prototype phase.
  Date/Author: 2025-12-17 / GPT-5.1-Codex-Max.
- Decision: Stage Python client scripts via a `python_clients` build target (included in presets) and add server-focused build presets for repeatable artifacts.
  Rationale: Integration tests and packaging can rely on the staged bundle without assuming a source checkout layout, and the presets provide a single command to build the network server plus clients.
  Date/Author: 2025-12-17 / GPT-5.1-Codex-Max.

## Outcomes & Retrospective

To be filled once v0.0.2 work completes; summarize the demonstrated architecture leap and remaining gaps.

## Context and Orientation

The current CLI (`tools/jubectl/main.cpp`) operates directly on `storage::SimpleStore`, which wraps the in-process BTree and persistence stack. Server-side scaffolding lives in `src/server/` with `TransactionReceiver`, `Worker`, and `Server` orchestrating keyed operations over `storage::btree::BTree`. Transaction shapes live in `src/txn/transaction_request.h` and transaction contexts in `src/txn/transaction_context.h`. The length-prefixed JSON wire protocol is implemented in `src/server/network_server.cpp` and consumed by both the Python client bundle under `tools/clients/python/` and the remote-aware `jubectl`. A `jubildb_server` bootstrap (`src/server/main.cpp`) wires config, storage initialization, worker pools, and the network adapter. Integration tests that exercise the new network path are still pending.

## Plan of Work

Begin by locking down a minimal, documented wire contract using length-prefixed JSON frames. Each request contains a transaction id, operations array, and optional value fields; each response echoes the id, final state, and per-operation results. Add documentation for this contract in `docs/` alongside this plan. Prototype the protocol in Python to validate framing, encoding of bytes/string/int values, and error handling before touching the C++ server.

Extend the C++ server by adding a network adapter (e.g., `src/server/network_server.{h,cpp}`) that owns a blocking TCP listener and connection handler threads. Each connection reads size-prefixed JSON frames, validates them into `txn::TransactionRequest`, and forwards them to `Server::SubmitTransaction`. Responses collect from `Server::DrainCompleted()` and write back the serialized `TransactionResult` with success flags and values. Keep the adapter small and defer advanced features (TLS, multiplexing) to later versions.

Wire configuration through the existing `ConfigLoader` (if available) or a minimal config struct to set listen host/port and worker counts. Provide a server entry point binary (e.g., `src/server/main.cpp`) that instantiates the storage engine and network adapter, mirroring `jubectl`’s initialization of `SimpleStore`.

Implement a Python client library and CLI script under `tools/clients/python/` that constructs the JSON envelopes, connects via TCP, and prints responses. Keep dependencies standard-library only. Provide helper methods for `set`, `get`, `del`, and a generic transaction call that accepts multiple operations. This workstream only depends on the envelope definition and can progress in parallel with server work; bundle the scripts via the `python_clients` target so integration tests and distributions can rely on a staged copy under `build/<preset>/python_clients/`.

Refactor `jubectl` to optionally use the network path. If parity requires, add a `--remote <host:port>` flag or a new subcommand (`txn`) that packages the same JSON requests. Preserve existing on-disk behavior by default to avoid breaking current users while enabling side-by-side validation. `jubectl` changes depend on the envelope definition and the Python client’s schema agreement, but not on server readiness.

Add integration tests under `tests/` that start the C++ server (likely via a spawned process or in-process bootstrap), run a short Python client script to issue `set/get/del`, and assert correct responses. Include tests for invalid frames and concurrent transactions where possible. Document the full workflow in `docs/` to keep newcomers aligned. Workstreams can advance in parallel where dependencies allow: protocol documentation and Python client prototyping can proceed alongside server adapter development; `jubectl` wiring can start once the request/response envelope is frozen; integration test scaffolding can begin in tandem with server bootstrap and be finalized once handlers stabilize. Explicit constraints: (a) Envelope must be frozen before client/Jubectl milestones merge; (b) Server adapter must exist before end-to-end tests assert results; (c) Bootstrap wiring can start as soon as adapter interfaces are stubbed; (d) Python client scripts are staged during builds via `python_clients`, ensuring tests can consume packaged copies even when run from the build tree.

## Concrete Steps

1. Document the wire contract in `docs/` (e.g., `docs/txn-wire-v0.0.2.md`), including request/response JSON schemas, framing rules (uint32 length prefix, network byte order), and examples for `set/get/del`. **Status:** complete — see `docs/txn-wire-v0.0.2.md`. **Blocks:** none; **Unblocks:** steps 2, 3, 6.
2. Create a Python client module `tools/clients/python/jubilant_client.py` with functions `connect`, `send_transaction`, and helpers for `set/get/del`. Add a simple CLI `tools/clients/python/jubectl_client.py` to exercise these calls. This can run in parallel with step 4 as long as both honor the same envelope. **Status:** complete — module and CLI landed under `tools/clients/python/`. **Blocks:** depends on step 1; **Unblocks:** step 8 (client half).
3. Prototype against a temporary echo server (could be a Python stub) to confirm framing; then retire stub once C++ server adapter is ready. This can proceed concurrently with documentation authoring. **Status:** complete — `tools/clients/python/echo_stub.py` implements the stub with a framing probe in `tools/clients/python/framing_probe.py` to exercise the set/get/del examples. **Blocks:** depends on step 1; **Unblocks:** early validation for steps 2 and 4.
4. Implement `src/server/network_server.{h,cpp}` for TCP accept and per-connection loops, converting frames into `txn::TransactionRequest` and emitting `TransactionResult` frames. Integrate with `src/server/server.{h,cpp}` through `SubmitTransaction` and `DrainCompleted`. Coordinate with step 2 to keep request/response JSON aligned. **Blocks:** depends on step 1; **Unblocks:** steps 5 and 8 (server half).
5. Add a bootstrap target `src/server/main.cpp` (or extend an existing one) that loads configuration, initializes storage (via `storage::SimpleStore` or the underlying BTree), starts the worker pool, and hands sockets to the network adapter. This can start while step 4 is underway because the bootstrap wires to the adapter interface. **Blocks:** depends on adapter interface definition from step 4 (stub acceptable); **Unblocks:** step 8 environment setup.
6. Update `tools/jubectl/main.cpp` to detect `--remote` or a new `txn` command. When remote is selected, reuse the JSON envelope helpers (consider a small shared C++ helper under `tools/jubectl/` or `src/client/`). Kick off once the envelope is frozen (steps 1–2) even if the server is still stabilizing. **Status:** complete — `--remote` now swaps `set/get/del` onto the JSON transport and a `txn` command submits JSON files through a shared framing helper. **Blocks:** depends on step 1 (and ideally step 2 for shared helpers); **Unblocks:** step 8 (jubectl half).
7. Add CMake targets for the network server binary and ensure presets build the Python client scripts as auxiliary artifacts if needed. This can happen alongside steps 4–6. **Status:** complete — `python_clients` now stages the Python bundle into `build/<preset>/python_clients/` and new build presets (`dev-debug-server`, `dev-release-server`) build the network server binary alongside the bundle. **Blocks:** depends on knowing paths from steps 2, 4, 6; **Unblocks:** CI-ready builds for step 8.
8. Write integration tests (C++ or Python) that start the server on a loopback port, run a sequence of `set/get/del` transactions via the Python client and `jubectl` remote mode, and verify responses plus BTree state on disk. Begin scaffolding early (while 4–6 progress) and finish once handlers stabilize. **Blocks:** depends on steps 2, 4, 5, 6 (for full end-to-end); can scaffold earlier using stubs from 3 and 4.
9. Update documentation (`docs/README.md` and this plan) with any discoveries, plus a short quickstart for running the server and client together. Keep this in lockstep with all streams to capture drift. **Blocks:** depends on outcomes of steps 2, 4, 6, 8; can be iteratively updated as each step lands.

## Validation and Acceptance

Acceptance hinges on an end-to-end demo: from the repository root, configuring with `cmake --preset dev-debug`, building, and launching the server with a sample config must allow a Python client to perform `set/get/del` and receive matching responses. A `jubectl --remote` invocation should produce the same results against the same server. Integration tests should pass and fail before the network path is added. Manual validation should show logs or stdout proving transaction ids and operation results echo back as expected. Error handling should return meaningful error responses for malformed JSON or missing keys.

## Idempotence and Recovery

Running the server/client repeatedly should be safe when pointing at a throwaway data directory; initialization steps should create directories if absent. The length-prefixed framing allows reconnecting cleanly after a disconnect. If integration tests leave stray processes, provide cleanup instructions (kill server by pid or ensure shutdown hooks join threads). Configurable ports avoid conflicts when rerunning.

## Artifacts and Notes

Capture sample request/response pairs in the wire contract doc, and record any divergence between Python and C++ parsing. Keep command transcripts (server start, client calls) brief and updated as behaviors change. Update this section as new evidence emerges during implementation.

## Interfaces and Dependencies

Define request JSON with fields: `txn_id` (uint64), `operations` array where each entry has `type` (`get`|`set`|`del`), `key` (string), and optional `value` holding `{kind: "bytes"|"string"|"int", data: base64|string|int64}`. Responses include `txn_id`, `state` (`committed`|`aborted`), and `operations` results mirroring the request order with `success` flag and optional `value`. The C++ network adapter should expose `Start()`/`Stop()` to manage listener lifecycle and use existing `Server::SubmitTransaction`/`DrainCompleted`. Python client depends only on `socket`, `json`, `struct`, and `argparse`.

_Update (2025-12-17, GPT-5.1-Codex-Max): Recorded completion of the server bootstrap binary (step 5) and linked documentation so newcomers can start the network adapter with a TOML config._
_Update (2025-12-17, GPT-5.1-Codex-Max): Completed step 7 by adding the `python_clients` staging target, server build presets, and documentation updates so builds surface the network artifacts needed for integration tests._
